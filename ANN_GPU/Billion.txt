Billion-scale similarity search with GPUs, Fais
https://arxiv.org/pdf/1702.08734

Abstract
	- Big Picture: use GPU to handle vector database systems
	- Bottlenecks: algorithms that expose less parallelism or memory hierarchy that GPU can take advantage of
	- Solution: faster k-selection design, faster nearest neighbor implementation
	
Introduction
	- Metadata for images & videos to index & search is not often available -> ML & DL algorithms are used to interpret & classify data
	- data embeddings are usually real-value, high-dimensional vectors
	- search by numerical similarity
	- expensive operation: computing k-NN graph
		- directed graph
		- each vector is node
		- each edge connects node to k nearest neighbors
	- state-of-the-art methods like NN-Descent are too memory intensive
	- issues: curse of dimensionality
	- approaches:
		- internal compressed reprentation of vectors
			- good for memory-limited devices like GPUs
			- ex. binary codes, quantization methods
	- paper focuses on product quantization -based codes 
		- original: IVFADC
		- most new improvements difficult to implement efficiently on GPU
	- paper makes contributions:
		- GPU k-selection algorithm
		- k-nearest neighbor search on GPU
		- experimental evidence
Problem Statement
	- query vector: x
	- collection: y_i, 0 <= i <= l
	- look for: k nearest neighbors of x in terms of L2 distance

	Batching
	- searches usually performed in batches of n_q query vectors in parallel
	Exact Search
	- use decomposition: x^2+y^2-2x.y -> bottleneck on third term = XY*
	Compressed-domain search
	- focus on ANN search
	- consider IVFADC indexing structure
		- y encoded as q(y)=q_1(y) + q2_(y-q_1(y)) (q_1 and q_2 are quantizers)
			- q_1 is coarse quantizer
			- q_2 is coarse quantizer
		- Vectors are preselected based on first-level quantizer
			- multi-probe parameter T is number of coarse-level centroids we consider
		- Does another search based on based on q(y) on subset of vectors
		- data structure: inverted file
			- groups vectors into inverted lists (each with same q_1)
		- bottleneck: linearly scanning T lists
	Quantizers
		- q_1: relatively low number of repeat values
		- q_2: can have longer codes
	Product quantizer:
		- use on q2
		- splits y into b sub-vectors
		- each sub-vector quantized with own quantizer
		- quantization value is concatenation of sub-vectors

GPU: Overview and K-Selection
Architecture
	- Nvidia GPU: general-purpose computer
	- warp: CUDA threads, 32-wide vector
	- lanes: individual threads in warp
		- lane ID: from 0-31
	- warp divergence: warp lanes taking different execution paths
	- block/cooperative thread array (CTA): user-configurable collection of 1-32 warps
	- shared memory: each block has up to 48 KiB
		- thread id: block-relative ID for each CUDA thread
	- streaming multiprocessor (SM): single core that block runs on
	- functional units: of SM includes ALUs, memory load/store, etc.
	- grid: group of blocks
		- each block assigned grid relative ID
	- kernel: unit of work scheduled by host CPU for GPU to execute
	- streams/events: ordering primitives that control kernel ordering
	- occupancy: of GPU is affected by:
		- number of blocks executing (depends on shared memory and register resources per block)
		- per-CUDA thread register determined at compilation time
		- shared memory usage can be chosen at runtime
	- global memory: how different blocks and kernels can communicate

Register file
	- shared and register tradeoff: lower occupancy but can increase performance (good)
	- warp shuffle instruction: allows lanes in GPU to exchange register data
	- lane-stride register array: common pattern to achieve above

k-selection
	- bottleneck: multiple passes over input
	- CPU solution: selection via max-heap
		- little data parallelism

Fast k-selection on GPU
- roofline performance model: memory or arithmetic throughput should be limiting factor
- in-register sorting primitive
	- Bitonic sort
- WarpSelect
	- entire state in registers
	- single pass over data
	- avoids cross-warp synchronization
- thread queue in each lane, stores t elements in registers
- warp queue for each warp
- invariants:
	- largest element of thread q is not in min-k
	- largest element of each thread is greater than all warp queue keys
	- all a_i seen so far in min-k are in some lane's thread queue or in warp queue
- Lane j receives new element, attempts to insert in thread queue - if greater than greatest elem, then too large and can be rejected
- Otherwise reject previous greatest element, insert in thread queue
- Use warp ballot instruction to see if lane has violated second invariant 
	- If has, warp uses ODD-MERGE to merge and sort thread and warp queues together
- parallel instructions for each 32 elements
	- read 32 elements, compare to thread heads
	- perform insertion sort
	- sort and merge queues

Computation Layout
- distance computation: matrix multiplication
- computing the PQ with lookup tables: 256 * d multiply-adds and n * b lookup-adds
- GPU Implementation
	- inverted lists stored as two separate arrays
	- kernel responsible for scanning T closest inverted lists for each query + calclating vector pair distances with lookup table
	- Each pair of query in list can be processed independently -> assigned to each block
- Multi-GPU parallelism
	- Replication
	- Sharding


