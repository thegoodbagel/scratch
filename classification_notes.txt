Six common classification algorithms:

1) Logistic Regression
2) Decision Tree
3) Random Forest
4) Support Vector Machine (SVM)
5) Naive Bayes
6) K-Nearest Neighbors (KNN)

--- Logistic Regression ---
* Uses sigmoid function to generate probabilistic output
* Trained using log likelihood

--- Decision Tree ---
* Training phase: building best tree (high accuracy, avoid overfitting)
* Splits data recursively
* To determine best split, can use following metrics:
	- Gini Impurity
	- Information Gain
	- Gain Ratio

* Formula for Gini Impurity: 
	-> Calculate for each class: 1 - Sum(p_i^2)
	-> p_i is proportion ofclass i in pool 
	-> lower the better
* Formula for Entropy and Information Gain:
	-> Entropy = - Sum over each class(p log(p))
	-> Information Gain = how much entropy decreases after a split = Entropy(parent) - sum of Entropy of children
* Formula for Gain Ratio:
	-> Penalizes splits that creates many small branches: Information Gain / Split Information
	-> Split Information = how 'even' the data is split across branches
	-> = -Sum over splits(S_j/S log(S_j/S)
	-> Low when split is balanced, high when split is uneven
	-> Acts as penalty over splitting too much into attributes with many values


--- Random Forest ---
* Collection of decision trees, where each tree is trained on random subset of data and features
* At prediction time, an 'average' of votes is taken

* Bootstrap Aggregating (Bagging) is used to generate different training data sets, creating variation between trees
	-> Repeatedly samples randomly with replacement from training set
* Instead of considering all features, RF randomly selects a subset of features 
	-> Trees are less correlated
* Once all trees are built, each tree predicts and class and a majority vote is taken
* Benefits of random forest include better generalization (less overfitting), handles high dimensions well, but is slower

--- Support Vector Machines ---
* Tries to separate data in classes as much as possible by finding hyperplane with largest possible margin
* Uses 'kernel trick' for non-linear datasets

--- Naive Bayes ---
* Assumes all features are independent given class C
  
